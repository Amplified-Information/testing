module "shared" {
  source = "../shared"
  
  env  = "$ENV"
  aws_key_bastion = "$ENV-bastion"
  aws_key_internal = "$ENV"
  domain_name = "$DOMAIN_NAME"
  ebs_volume_id = "$EBS_VOLUME_ID"
  ssl_cert_arn = "$SSL_CERT_ARN"
}


resource "aws_instance" "bastion_$ENV" {
  ami           = module.shared.ami
  instance_type = "t3.nano"
  key_name      = module.shared.aws_key_bastion
  availability_zone = module.shared.aws_az
  
  associate_public_ip_address = true                            # bastion needs a public IP
  
  subnet_id              = module.shared.aws_subnet_public_id   # PUBLIC subnet
  private_ip             = module.shared.fixed_ip_bastion

  vpc_security_group_ids = [
    module.shared.allow_web_egress_id,
    module.shared.allow_web_ingress_id,
    module.shared.allow_internal_vpc_id,
    # module.shared.allow_internal_private_subnet_id,
    module.shared.allow_ssh_ingress_id                          # SSH is allowed to Bastion
    # module.shared.allow_ssh_from_public_subnet_id,
    # module.shared.allow_8090_from_internet_id
  ]

  tags = {
    Name = "${module.shared.env}.bastion"
  }

  user_data = <<-EOF
    #!/bin/bash
    ${module.shared.install_base}
    ${module.shared.install_bastion}

    # for good measure
    reboot
  EOF
}



resource "aws_instance" "proxy_$ENV" {
  ami           = module.shared.ami
  instance_type = "t3.nano"
  key_name      = module.shared.aws_key_internal
  availability_zone = module.shared.aws_az

  subnet_id              = module.shared.aws_subnet_private_id # aws_subnet_public_id # PUBLIC subnet
  private_ip             = module.shared.fixed_ip_proxy

  vpc_security_group_ids = [
    module.shared.allow_web_egress_id,
    module.shared.allow_web_ingress_id,
    # module.shared.allow_internal_vpc_id,
    module.shared.allow_internal_private_subnet_id,
    # module.shared.allow_ssh_ingress_id
    module.shared.allow_ssh_from_public_subnet_id,
    # module.shared.allow_8090_from_internet_id,
    # module.shared.allow_proxy_ingress_id,
    # module.shared.allow_monolith_egress_id,
    module.shared.allow_alb_ingress_id
  ]

  iam_instance_profile = module.shared.combined_iam_policy_name # combined IAM

  user_data = <<-EOF
    #!/bin/bash

    hostnamectl set-hostname proxy # script needs to know

    ${module.shared.install_base}
    ${module.shared.install_docker_runner}

    # for good measure
    reboot
  EOF

  tags = {
    Name = "${module.shared.env}.proxy"
  }
}




resource "aws_instance" "monolith_$ENV" {
  ami           = module.shared.ami
  instance_type = "t3.micro"
  key_name      = module.shared.aws_key_internal
  availability_zone = module.shared.aws_az

  subnet_id              = module.shared.aws_subnet_private_id
  private_ip             = module.shared.fixed_ip_monolith

  vpc_security_group_ids = [
    module.shared.allow_web_egress_id,
    module.shared.allow_email_egress_id,
    module.shared.allow_web_ingress_id,
    # module.shared.allow_internal_vpc_id,
    module.shared.allow_internal_private_subnet_id,
    # module.shared.allow_ssh_ingress_id,
    module.shared.allow_ssh_from_public_subnet_id,
    # module.shared.allow_8090_from_internet_id, 
    module.shared.allow_proxy_ingress_id,
    # module.shared.allow_monolith_egress_id,
    module.shared.allow_hedera_rpc_egress_id,
  ]

  iam_instance_profile = module.shared.combined_iam_policy_name # combined IAM

  user_data = <<-EOF
    #!/bin/bash

    hostnamectl set-hostname monolith # script needs to know

    ${module.shared.install_base}
    ${module.shared.install_docker_runner}

    # for good measure
    reboot
  EOF

  tags = {
    Name = "${module.shared.env}.monolith"
  }
}


resource "aws_instance" "data_$ENV" {
  ami           = module.shared.ami
  instance_type = "t3.nano"
  key_name      = module.shared.aws_key_internal
  availability_zone = module.shared.aws_az                      # must be the same as the EBS volume

  subnet_id              = module.shared.aws_subnet_private_id
  private_ip             = module.shared.fixed_ip_data

   vpc_security_group_ids = [
    module.shared.allow_web_egress_id,
    module.shared.allow_web_ingress_id,
    # module.shared.allow_internal_vpc_id,
    module.shared.allow_internal_private_subnet_id,
    # module.shared.allow_ssh_ingress_id,
    module.shared.allow_ssh_from_public_subnet_id,
    # module.shared.allow_8090_from_internet_id,
    # module.shared.allow_proxy_ingress_id,
    # module.shared.allow_monolith_egress_id,
    module.shared.allow_bastion_db_id # Warning: allow 5432 from bastion
  ]

  iam_instance_profile = module.shared.combined_iam_policy_name # combined IAM

  user_data = <<-EOF
    #!/bin/bash

    hostnamectl set-hostname data # script needs to know

    ${module.shared.install_base}
    ${module.shared.install_docker_runner}
    ${module.shared.install_data}

    # for good measure
    reboot
  EOF

  tags = {
    Name = "${module.shared.env}.data"
  }
}
# Attach the EBS volume to the data_$ENV instance
resource "aws_volume_attachment" "data_${ENV}_volume_attachment" {
  device_name = "/dev/xvdf"
  instance_id = aws_instance.data_$ENV.id
  volume_id   = module.shared.ebs_volume_id
}




#####
# Outputs
#####
output "proxy_${ENV}_public_dns" {
  description = "Public DNS name of the proxy_$ENV instance"
  value       = aws_instance.proxy_$ENV.public_dns
}

output "proxy_${ENV}_private_dns" {
  description = "Private DNS name of the proxy_$ENV instance"
  value       = aws_instance.proxy_$ENV.private_dns
}

output "monolith_${ENV}_private_dns" {
  description = "Private DNS name of the monolith_$ENV instance"
  value       = aws_instance.monolith_$ENV.private_dns
}

output "data_${ENV}_private_dns" {
  description = "Private DNS name of the data_$ENV instance"
  value       = aws_instance.data_$ENV.private_dns
}

output "bastion_${ENV}_public_dns" {
  description = "Public DNS name of the bastion_$ENV instance"
  value       = aws_instance.bastion_$ENV.public_dns
}

output "bastion_${ENV}_private_dns" {
  description = "Private DNS name of the bastion_$ENV instance"
  value       = aws_instance.bastion_$ENV.private_dns
}

output "alb_name" {
  value = aws_lb.alb_$ENV.name
}

output "alb_arn" {
  value = aws_lb.alb_$ENV.arn
}

output "alb_dns_name" {
  value = aws_lb.alb_$ENV.dns_name
}
















#####
# Application Load Balancer (ALB) for proxy
#####
resource "aws_lb" "alb_$ENV" {
  name               = "${module.shared.env}-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [
    module.shared.allow_web_ingress_id,
    module.shared.allow_web_egress_id,
    module.shared.allow_alb_egress_id
  ]
  subnets            = [
    module.shared.aws_subnet_public_id,
    module.shared.aws_subnet_public_id_2
  ]

  enable_deletion_protection = false

  tags = {
    Name = "${module.shared.env}-alb"
  }
}

# Create a target group for the proxy_$ENV instance
resource "aws_lb_target_group" "alb_proxy_target_group_$ENV" {
  name        = "${module.shared.env}-alb-proxy-tg"
  port        = 8090
  protocol    = "HTTP"
  protocol_version = "HTTP1" # HTTP1 (default), HTTP2, GRPC
  vpc_id      = module.shared.vpc_id

  health_check {
    path                = "/health"
    protocol            = "HTTP"
    interval            = 30
    timeout             = 5
    healthy_threshold   = 2
    unhealthy_threshold = 2
  }

  tags = {
    Name = "${module.shared.env}-alb-proxy-tg"
  }
}

# ALB listener uses HTTPS on port 443
resource "aws_lb_listener" "alb_listener_443" {
  load_balancer_arn = aws_lb.alb_$ENV.arn
  port              = 443
  protocol          = "HTTPS"

  ssl_policy        = "ELBSecurityPolicy-2016-08"
  certificate_arn   = module.shared.ssl_cert_arn

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.alb_proxy_target_group_$ENV.arn
  }
}

# Register the proxy_$ENV instance with the target group
resource "aws_lb_target_group_attachment" "proxy_${ENV}_attachment" {
  target_group_arn = aws_lb_target_group.alb_proxy_target_group_$ENV.arn
  target_id        = aws_instance.proxy_$ENV.id
  port             = 8090
}















#####
# Route 53 - connect the ALB to the domain
#####

# Add an alias record pointing to the ALB
resource "aws_route53_record" "alb_alias" {
  zone_id = "Z07868573V3HLWHKP9WV6" # yes, this is fixed
  name    = "${module.shared.domain_name}"
  type    = "A"

  alias {
    name                   = aws_lb.alb_$ENV.dns_name
    zone_id                = aws_lb.alb_$ENV.zone_id
    evaluate_target_health = true
  }
}
